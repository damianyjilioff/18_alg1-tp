{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/damianyjilioff/18_alg1-tp/blob/master/Clase_02_campus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "is_executing": false
        },
        "id": "4Kh4VZCwvLAo"
      },
      "source": [
        "### Introducción a la Investigación Operativa y la Optimización\n",
        "\n",
        "### • Clase 2  - Algoritmos de descenso\n",
        "\n",
        "**Nazareno Faillace Mullen - Departamento de Matemática, FCEN, UBA**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {},
        "id": "poI7lbYdvLAs"
      },
      "source": [
        "### Algoritmos de descenso"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {},
        "id": "T3W4RmVfvLAt"
      },
      "source": [
        "_Idea_: a partir de un punto obtenido, escoger una dirección para dar el próximo paso\n",
        "\n",
        "__Definición (dirección de descenso)__: sean $f:\\mathbb{R}^n \\rightarrow \\mathbb{R}$, $\\bar{x}\\in\\mathbb{R}^n$ y $d\\in\\mathbb{R}^n-\\{0\\}$, diremos que $d$ es una dirección de descenso para $f$ a partir de $\\bar{x}$ si existe $\\delta>0$ tal que $f(\\bar{x}+td)<f(\\bar{x}) \\quad \\forall t\\in(0,\\delta)$ \n",
        "\n",
        "__Teorema__: Si $\\nabla f(\\bar{x})d < 0$, entonces $d$ es dirección de descenso para $f$ a partir de $\\bar{x}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {},
        "id": "CEgdHaETvLAu"
      },
      "source": [
        "### __Algoritmo de descenso básico__\n",
        "\n",
        "Dados: $f,\\; x_0 \\in \\mathbb{R}^n,\\; \\varepsilon>0,\\; k_{MAX}>0$ <br>\n",
        "k = 0 <br>\n",
        "REPETIR mientras $\\nabla f(x_k) > \\varepsilon$ y $k<k_{MAX}$ : <br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp; Calcular $d_k$ tal que $\\nabla f(x_k)^Td_k < 0$ <br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp; Escoger $t_k>0$ tal que $f(x_k+t_kd_k)<f(x_k)$ <br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp; Hacer $x_{k+1}=x_k + t_kd_k$ <br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp; $k = k+1$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8iaPFx6vLAv"
      },
      "source": [
        "## Método del gradiente - Funciones cuadráticas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YNVLBWovLAw"
      },
      "source": [
        "Las funciones cuadráticas pueden escribirse en la forma:\n",
        "\n",
        "$$f(x) = \\frac{1}{2}x^T A x + bx + c$$\n",
        "\n",
        "En este caso, el gradiente se calcula fácilmente: <br>\n",
        "$$\\nabla f(x) = Ax + b$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkcQgOcOvLAx"
      },
      "source": [
        "En el método del gradiente, $d_k = -\\nabla f(x_k)$\n",
        "\n",
        "Si $A$ es definida positiva, se puede demostrar que $\\varphi(t) = f(x_k + td_k)$ alcanza mínimo en:\n",
        "$$ t^\\ast = \\dfrac{d_k^T d_k}{d_k^T A d_k}$$\n",
        "\n",
        "Entonces, en cada iteración se puede calcular la longitud del paso óptimo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPGLm0bBvLAx"
      },
      "source": [
        "### Pseudocódigo de Método de gradiente para funciones cuadráticas:\n",
        "\n",
        "Método_gradiente($A$,$b$,$x_0$, $max\\_iter$):<br>\n",
        "&nbsp; &nbsp; $k$ = $0$ <br>\n",
        "&nbsp; &nbsp; $x_k$ = $x_0$ <br>\n",
        "&nbsp; &nbsp; $d_k$ = $-Ax_0-b$ &nbsp; &nbsp; `# Dirección del primer paso` <br> \n",
        "&nbsp; &nbsp; while $k\\leq max\\_iter$ and $\\lVert d_{k}\\rVert$ $> 10^{-8}$: <br>\n",
        "&nbsp; &nbsp; &nbsp; &nbsp; $t = \\dfrac{d^T d}{d^T A d}$ &nbsp; &nbsp; `# Determino la longitud del paso` <br>\n",
        "&nbsp; &nbsp; &nbsp; &nbsp; $x_{k}$ = $x_k + td_k$ &nbsp; &nbsp; `# Calculo el siguiente punto de la iteración (\"doy el paso\")`<br> \n",
        "&nbsp; &nbsp; &nbsp; &nbsp; $d_{k}$ = $- Ax_{k+1}-b$  &nbsp; &nbsp; `# Dirección del próximo paso` <br>\n",
        "&nbsp; &nbsp; &nbsp; &nbsp; $k$ = $k+1$ <br>\n",
        "&nbsp; &nbsp; DEVOLVER $x_{k}$\n",
        "\n",
        "**Obs:** utilizar `np.linalg.norm(v)` para calcular la norma del vector `v`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoZ1fvZtvLAz"
      },
      "source": [
        "## Ejercicio\n",
        "\n",
        "1. Implementar el Método del gradiente para funciones cuadráticas en base al pseudocódigo anterior. Además de devolver la aproximación al mínimo, que también devuelva la cantidad de iteraciones.\n",
        "2. Para la función cuadrática $f(x)=\\frac{1}{2}x^TAx$ con $A$ dada más abajo, correr el Método del Gradiente: <br>\n",
        "a) como fue implementado en el punto 1 <br>\n",
        "b) con $\\frac{1}{2}$ de longitud del paso óptimo <br>\n",
        "c) en cada iteración multiplicar $t$  longitud de paso por un número aleatorio en (0,1] (`np.random.rand()` devuelve un float aleatorio) <br>\n",
        "Probar con $x_0$ el vector de 1's (`np.ones(10)`) y un máximo de 10000 iteraciones. ¿Cuál se desempeña mejor?\n",
        "3. Para $B$ dada más abajo, correr el Método del Gradiente con la implementación del Punto 1 para $f(x)=\\frac{1}{2}x^T B x$, ¿en cuántas iteraciones termina? ¿Por qué?  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZebH2Ai5vLA0",
        "outputId": "090a7ba8-f1b1-4a17-a8f2-4a29df0f996c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(array([-13884.87465513,  -7847.53784855,  -8149.40468888, -10262.47257118,\n",
            "        -9356.8720502 ,  -8753.13836954,  -9658.73889052,  -8149.40468888,\n",
            "       -13884.87465513,  -7545.67100822]), 10001)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "A = np.array([[8, 3, 3, 6, 5, 4, 4, 3, 6, 3],\n",
        "             [3, 4, 2, 2, 2, 1, 3, 3, 3, 2],\n",
        "             [3, 2, 5, 2, 1, 2, 4, 2, 4, 1],\n",
        "             [6, 2, 2, 6, 3, 2, 4, 2, 4, 2],\n",
        "             [5, 2, 1, 3, 5, 4, 1, 2, 4, 3],\n",
        "             [4, 1, 2, 2, 4, 5, 1, 2, 5, 2],\n",
        "             [4, 3, 4, 4, 1, 1, 6, 2, 4, 2],\n",
        "             [3, 3, 2, 2, 2, 2, 2, 4, 4, 2],\n",
        "             [6, 3, 4, 4, 4, 5, 4, 4, 8, 3],\n",
        "             [3, 2, 1, 2, 3, 2, 2, 2, 3, 4]])\n",
        "\n",
        "B = 2*np.eye(10)\n",
        "def Metodo_gradiente(A,b,x_0,max_iter):\n",
        "  k=0\n",
        "  x_k=x_0\n",
        "  d_k= -A@x_0 - b\n",
        "  while k<=max_iter and np.linalg.norm(d_k)>10**(-8):\n",
        "    t= (d_k@d_k)/(d_k@A@d_k)\n",
        "    x_k= x_k + t*d_k\n",
        "    k=k+1\n",
        "  return (x_k,k)\n",
        "  \n",
        "print(Metodo_gradiente(A,np.ones(10),np.ones(10),10000))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUa-QAh8vLA2"
      },
      "source": [
        "## ¿Y si $f$ no es cuadrática?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {},
        "id": "FFn4riBGvLA2"
      },
      "source": [
        "## Métodos de búsqueda unidireccional"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {},
        "id": "Cd5976CwvLA2"
      },
      "source": [
        "Dados $x,d\\in\\mathbb{R}^n$, lo que nosotros querríamos hacer es resolver el siguiente problema:\n",
        "<center>minimizar $f(x+td)$</center>\n",
        "<center>sujeto a: $t\\geq0$</center>\n",
        "Naturalmente, es un objetivo ambicioso pues no resulta una tarea fácil salvo que $f$ cumpla con características muy específicas. Veremos algoritmos que permiten aproximar a la solución de ese problema."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {},
        "id": "TnyubBYGvLA3"
      },
      "source": [
        "## Búsqueda inexacta - Condición de Armijo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {},
        "id": "afDMwYudvLA3"
      },
      "source": [
        "A diferencia de la sección áurea, este algoritmo no busca minimizar $\\varphi(t)=f(x_k+td)$ sino encontrar $t$ tal que haya una buena reducción en la función objetivo. Más específicamente, dados $x_k\\in\\mathbb{R}^n$, $d$ dirección de descenso y $\\eta\\in(0,1)$ busca $t$ que cumpla:\n",
        "$$f(x_k+td) \\leq f(x_k+td_k) + \\eta \\langle\\nabla f(x_k) ,d\\rangle t = \\varphi(0) + \\eta \\varphi^\\prime(0)t$$\n",
        "es decir, la reducción debe ser proporcional al tamaño del paso.\n",
        "\n",
        "Para que el paso no sea demasiado pequeño, agregamos una condición más a $t$. Dado $\\beta>1$, $t$ no es un paso pequeño si cumple:\n",
        "$$\\varphi(\\beta t) > \\varphi(0) + \\eta \\varphi^\\prime(0)(\\beta t) $$\n",
        "Es decir, que si incrementamos $t$ en un factor $\\beta$, deja de cumplir la condición de Armijo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {},
        "id": "JURMAEQEvLA3"
      },
      "source": [
        "### Interpretación gráfica (Armijo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OfwCt5RvLA4"
      },
      "source": [
        "![armijo.PNG](attachment:armijo.PNG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {},
        "id": "3mCO2uJYvLA4"
      },
      "source": [
        "__Algoritmo de Búsqueda de Armijo__\n",
        "\n",
        "Dados: $f\\colon\\mathbb{R}^n\\rightarrow\\mathbb{R},\\;\\bar{x}\\in\\mathbb{R}^n$, $d\\in \\mathbb{R^n}$ dirección de descenso, $\\eta\\in(0,1)$, $\\beta > 1$<br>\n",
        "\n",
        "$t=1$<br>\n",
        "SI $f(\\bar{x}+td) \\leq f(\\bar{x})+\\eta t\\nabla f(\\bar{x})^Td$: <br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp; REPETIR mientras $f(\\bar{x}+td) \\leq f(\\bar{x})+\\eta t\\nabla f(\\bar{x})^Td$: <br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $t=\\beta t$ <br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp; DEVOLVER $\\dfrac{t}{\\beta}$ <br>\n",
        "SI NO: <br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp; REPETIR mientras $f(\\bar{x}+td) > f(\\bar{x})+\\eta t\\nabla f(\\bar{x})^Td$: <br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $t= \\dfrac{t}{\\beta}$ <br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp; DEVOLVER $t$\n",
        "\n",
        "Valores usuales para los parámetros (Luenberger) : $\\beta = 2\\text{ (o $\\beta = 10$)},\\; \\eta=0.2$ <br>\n",
        "Otros valores usuales (Nocedal) : $\\beta = 7, \\;\\eta = 0.45$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {},
        "id": "d_Ubx7DlvLA5"
      },
      "source": [
        "## Búsqueda inexacta - Condiciones de Wolfe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {},
        "id": "MQM2yzSOvLA5"
      },
      "source": [
        "Wolfe brinda otra condición que acote inferiormente a $t$, la condición de curvatura:\n",
        "$$\\nabla f (x_k + td)^Td \\geq \\zeta \\nabla f(x_k)^T d$$\n",
        "donde $\\zeta\\in(\\eta,1)$ con $\\eta$ siendo la constante de la condición de Armijo. El lado izquierdo es $\\varphi'(t)$, por lo que la condición impone que la pendiente de $\\varphi$ en $t$ sea mayor que $\\zeta$ veces la pendiente inicial.\n",
        "\n",
        "Si $\\varphi'$ es muy negativa $\\Rightarrow$ se puede decrecer mucho en esta dirección <br>\n",
        "Si $\\varphi'$ no es muy negativa o es positiva $\\Rightarrow$ terminar la búsqueda lineal, no se pueden lograr (muchas) mejoras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {},
        "id": "LVfG78MsvLA5"
      },
      "source": [
        "__Condiciones de Wolfe:__\n",
        "\n",
        "$$\\begin{array}{rcl} f(\\bar{x}+\\bar{t}d) &\\leq& f(\\bar{x})+c_1\\bar{t}\\nabla f(\\bar{x})^Td \\quad \\text{(la condición de Armijo)} \\\\\n",
        "\\nabla f (\\bar{x} + \\bar{t}d)^Td &\\geq& c_2 \\nabla f(\\bar{x})^T d \\quad \\text{(es decir: $\\varphi^\\prime(t)\\geq c_2 \\varphi^\\prime(0)$)} \\end{array} $$\n",
        "Con $0<c_1<c_2<1$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {},
        "id": "k__L2_izvLA6"
      },
      "source": [
        "### Interpretación gráfica (Condiciones de Wolfe)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1lhUVsvvLA6"
      },
      "source": [
        "![wolfe.png](attachment:wolfe.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {},
        "id": "2ULAhLm6vLA6"
      },
      "source": [
        "__Algoritmo de Búsqueda de Wolfe__\n",
        "\n",
        "Dados: $f,\\; \\bar{x}\\in\\mathbb{R}^n,\\; d$ dirección de descenso,$\\; 0<c_1<c_2<1$ <br>\n",
        "Definir $\\alpha = 0,\\; t=1,\\; \\beta = +\\infty$ (`beta=np.inf`) <br>\n",
        "REPETIR<br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp; SI $f(\\bar{x}+td) > f(\\bar{x})+c_1t\\nabla f(\\bar{x})^Td$ :<br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Definir $\\beta=t,\\; t=\\frac{1}{2}(\\alpha+\\beta)$<br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp; De lo contrario, si $\\nabla f (\\bar{x} + td)^Td < c_2 \\nabla f(\\bar{x})^T d$:<br> \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Definir $\\alpha=t ,\\; t=\\begin{cases} 2\\alpha \\quad \\text{si } \\beta=+\\infty \\\\ \\frac{1}{2}(\\alpha+\\beta) \\quad \\text{c.c.} \\end{cases} $ <br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp; Si no: PARAR <br>\n",
        "DEVOLVER $t$\n",
        "\n",
        "Valores usuales para parámetros (Nocedal) : $c_1= 0.5,\\; c_2=0.75$ <br>\n",
        "El comando `break` de Python permite salir de un ciclo `while` o `for`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {},
        "id": "PD3pWhESvLA7"
      },
      "source": [
        "__Lema:__ sean $f:\\mathbb{R}^n\\rightarrow \\mathbb{R}$, $f\\in C^1$, $\\bar{x}\\in\\mathbb{R}^n$ y $d$ una dirección de descenso, entonces una de las siguientes dos situaciones pueden ocurrir con el método antes expuesto para las condiciones de Wolfe: <br>\n",
        "i) El procedimiento termina en una cantidad finita de pasos, devolviendo un valor $t$ que satisface las condiciones de Wolfe <br>\n",
        "ii) El procedimiento no termina: el parámetro $\\beta$ nunca toma un valor finito, $\\alpha$ se vuelve positivo en la primera iteración y se duplica con las iteraciones siguientes, y $f(x+td)\\rightarrow -\\infty$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {},
        "id": "TJ-8p6ILvLA7"
      },
      "source": [
        "# Métodos de Optimización Irrestricta "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {},
        "id": "vOSYxXoovLA7"
      },
      "source": [
        "## Método del Gradiente (o de Cauchy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {},
        "id": "dqkesDFBvLA7"
      },
      "source": [
        "En este método, se toma como dirección de descenso:\n",
        "$$d = -\\nabla f(x)$$\n",
        "Observar que, si $\\nabla f(x) \\neq 0$, efectivamente $d$ es dirección de descenso:\n",
        "$$\\nabla f(x)^T d = -\\left\\Vert \\nabla f(x) \\right\\Vert ^2 < 0$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {},
        "id": "9BB5_4QMvLA8"
      },
      "source": [
        "__Algoritmo del Método del Gradiente__\n",
        "\n",
        "Dados: $f, x^0 \\in \\mathbb{R}^n,\\; \\varepsilon>0,\\; k_{MAX}>0 $ <br>\n",
        "$k = 0$ <br>\n",
        "REPETIR mientras $\\lVert\\nabla f(x_{k})\\rVert > \\varepsilon$ y $k<k_{MAX}$ : <br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp; Definir $d_k = -\\nabla f(x_k)$ <br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp; Obtener $t_k>0$ tal que $f(x_k+t_kd_k)<f(x_k)$ (con Armijo o Wolfe) <br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp; Hacer $x_{k+1} = x_k + t_kd_k$ <br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp; $k = k+1$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {},
        "id": "VKj22thDvLA8"
      },
      "source": [
        "## Consignas\n",
        "\n",
        "Implementar una función para cada una de las siguientes tareas:\n",
        "* devolver la longitud del paso utilizando sección áurea\n",
        "* devolver la longitud de un paso que cumpla con la Condición de Armijo (+ condicion para que $t$ no sea demasiado pequeño)\n",
        "* [OPCIONAL] devolver la longitud de un paso que cumpla con las Condiciones de Wolfe\n",
        "* encontrar mínimo utilizando el Método del Gradiente\n",
        "\n",
        "La última tres deben tener `metodo` como un input opcional que permita elegir qué metodo utilizar (sección áurea, Armijo o Wolfe) para encontrar la longitud del paso. Deben imprimir la cantidad de iteraciones que realizaron, el minimizador y el valor de la función en el mismo. \n",
        "\n",
        "Aplicar Método del Gradiente sobre la función de Rosenbrock en 2 dimensiones. El minimizador de esta función es $x^*=(1,1)$ y $f(x^*)=0$\n",
        "\n",
        "También pueden probar con la función $f(x_1,x_2)= 2(e^{(-x_1^2-x_2^2)} - e^{(-(x_1-1)^2-(x_2-1)^2)})$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dM8OVV4vLA8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly\n",
        "import plotly.express as px\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import init_notebook_mode, iplot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SC-K0dA7vLA9"
      },
      "outputs": [],
      "source": [
        "def rosenbrock(x):\n",
        "    \"\"\"\n",
        "    minimiser : x = (1,..., 1)\n",
        "    \"\"\"\n",
        "    d = np.shape(x)[0]\n",
        "    return sum(100*(x[i+1]-x[i]**2)**2+(x[i]-1)**2 for i in range(d-1))\n",
        "\n",
        "def resta_exponenciales(x):\n",
        "    s1 = np.exp(-x[0] ** 2 - x[1] ** 2)\n",
        "    s2 = np.exp(-(x[0] - 1) ** 2 - (x[1] - 1) ** 2)\n",
        "    return (s1 - s2) * 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {},
        "id": "fMwJfIm-vLA9"
      },
      "source": [
        "Para graficar las funciones pueden usar la siguiente función `plot_function`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NL2smFT4vLA9"
      },
      "outputs": [],
      "source": [
        "def plot_fun(f, limites, points=None):\n",
        "    \"\"\"\n",
        "    f : función a graficar\n",
        "    limites : toma una tupla (x1,x2,y1,y2) de los límites del gráfico: grafica en el dominio [x1,x2] x [y1,y2]\n",
        "    points : lista de puntos a graficar sobre la superficie; se ingresa como una lista de tuplas (x,y,z) \n",
        "    \"\"\"\n",
        "    init_notebook_mode(connected=True)\n",
        "\n",
        "    x = np.linspace(limites[0], limites[1], 1000)\n",
        "    y = np.linspace(limites[2], limites[3], 1000)\n",
        "    X, Y = np.meshgrid(x, y)\n",
        "    Z = f((X, Y))\n",
        "    data = [go.Surface(x=x, y=y, z=Z)]\n",
        "    if points is not None:\n",
        "        for p in points:\n",
        "            data.append(go.Scatter3d(x=[p[0]], y=[p[1]], z=[p[2]], mode='markers'))\n",
        "    fig = go.Figure(data=data)\n",
        "    iplot(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Da43dCkgvLA9"
      },
      "outputs": [],
      "source": [
        "plot_fun(resta_exponenciales, limites=(-3,3,-3,3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwzBMV8GvLA-"
      },
      "source": [
        "Mientras que para graficar las curvas de nivel de una función, se puede utilizar la siguiente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9U_ycrKDvLA-"
      },
      "outputs": [],
      "source": [
        "%matplotlib notebook\n",
        "\n",
        "def curvas_nivel(f, limites, levels=10):\n",
        "    \"\"\" \n",
        "    Función que grafica curvas de nivel.\n",
        "    f : es la función a graficar (tiene que ir de R2 en R)\n",
        "    limites: es una lista o tupla de números: [a,b,c,d]. Va a graficar la función en el cuadrado [a,b] x [c,d]\n",
        "    levels : cantidad de curvas de nivel a graficar\n",
        "    \"\"\"\n",
        "    plt.figure()\n",
        "    x = np.linspace(limites[0], limites[1], 1000)\n",
        "    y = np.linspace(limites[2], limites[3], 1000)\n",
        "    X, Y = np.meshgrid(x, y)\n",
        "    Z = f((X, Y))\n",
        "    plt.contour(X,Y,Z, cmap='plasma', levels=levels)\n",
        "    plt.tight_layout()\n",
        "    plt.gca().set_aspect('equal', adjustable='box')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRxev42jvLA-"
      },
      "outputs": [],
      "source": [
        "curvas_nivel(resta_exponenciales, (-2,3,-2,3))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}